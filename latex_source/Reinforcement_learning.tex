\section{Reinforcement Learning Algorithms}
This work benchmarks several state-of-the-art Deep Reinforcement Learning algorithms. The following sections provide a detailed mathematical description of the selected off-policy, actor-critic algorithms that form the core of the experimental comparison.
\noindent

\subsubsection{Soft Actor-Critic (SAC)}
SAC is an off-policy actor-critic algorithm designed for continuous action spaces that optimizes a stochastic policy. Its distinguishing feature is the explicit incorporation of entropy maximization into the objective function, which promotes exploration and yields policies that are more robust to model inaccuracies and environmental perturbations. Unlike traditional RL algorithms that focus solely on reward maximization, SAC aims to maximize both the expected cumulative reward and the entropy of the policy distribution.
\noindent

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{SAC.png}
    \caption{SAC actor-critic architecture. The actor network outputs a stochastic policy parameterized by a mean and standard deviation, while two separate Q-networks (critics) estimate state-action values. The minimum of the two Q-values is used for policy updates to mitigate overestimation bias. Image from \footcite{Qiu2023}.}
    \label{fig:SAC}
\end{figure}

The objective function is:
\[
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} \left[ r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t)) \right]
\]
\noindent
where $\mathcal{H}$ is the entropy of the policy $\pi$, defined as $\mathcal{H}(\pi(\cdot|s_t)) = -\mathbb{E}_{a_t \sim \pi}[\log \pi(a_t|s_t)]$, and $\alpha$ is the temperature parameter, which controls the trade-off between reward exploitation and entropy-driven exploration. A higher $\alpha$ encourages more stochastic behavior, while a lower $\alpha$ biases the policy toward deterministic, reward-maximizing actions.
\noindent

\paragraph{Implementation Details}
The implementation of SAC is built upon the robust, industry-standard \textbf{Stable-Baselines3} library, which provides a highly optimized and well-tested version of the algorithm. The standard \texttt{SAC} class from this library is used directly, leveraging its PyTorch-based backend for efficient training and inference.
\noindent

SAC employs a soft Q-function, which incorporates the policy entropy into the value estimation. The soft Q-function is trained to minimize the soft Bellman residual:
\[
L(\theta_Q) = \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim D} \left[ \left( Q(s_t, a_t) - \left(r_t + \gamma V_{\bar{\psi}}(s_{t+1})\right) \right)^2 \right]
\]
\noindent
where $D$ is the replay buffer, $\gamma$ is the discount factor, and the soft state value function $V$ is defined as:
\[
V_{\text{soft}}(s_t) = \mathbb{E}_{a_t \sim \pi} [Q_{\text{soft}}(s_t, a_t) - \alpha \log \pi(a_t|s_t)]
\]
\noindent
To mitigate the positive bias inherent in Q-learning, SAC employs two independent Q-networks (implementing the Clipped Double-Q technique) and uses the minimum of the two target Q-values during the Bellman update. This conservative approach prevents overestimation from propagating through the learning process. Additionally, SAC can automatically tune the temperature parameter $\alpha$ during training by treating it as a constrained optimization problem, ensuring that the policy entropy remains above a target threshold.
\noindent

\subsubsection{Deep Deterministic Policy Gradient + PER (DDPG+PER)}
DDPG is an off-policy algorithm that concurrently learns a deterministic policy $\mu(s | \theta^\mu)$ and a Q-function $Q(s, a | \theta^Q)$. It extends the Deterministic Policy Gradient (DPG) theorem to deep neural networks, enabling continuous control in high-dimensional state spaces. Unlike stochastic policy methods, DDPG directly outputs a single action for each state, which can be advantageous in environments where deterministic behavior is preferred or when the action space is high-dimensional.
\noindent

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{DDPG.png}
    \caption{DDPG actor-critic architecture. The actor network produces a deterministic action, which is evaluated by the Q-network (critic). Both networks maintain slowly-updating target networks ($\mu'$ and $Q'$) for stable Bellman updates. During training, exploration noise is added to the actor's output. Image from \footcite{Qiu2023}.}
    \label{fig:DDPG}
\end{figure}

\begin{itemize}
    \item \textbf{Critic Update:} The critic is updated by minimizing the mean-squared Bellman error, analogous to Q-learning. Target networks ($Q'$ and $\mu'$) are used to stabilize training by providing consistent targets during the temporal difference update.
    \[
    L(\theta^Q) = \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim D} \left[ (y_t - Q(s_t, a_t | \theta^Q))^2 \right]
    \]
    \noindent
    where the target $y_t$ is given by:
    \[
    y_t = r_t + \gamma Q'(s_{t+1}, \mu'(s_{t+1}|\theta^{\mu'})|\theta^{Q'})
    \]
    \noindent
    The target networks are updated via soft updates (polyak averaging): $\theta' \leftarrow \tau \theta + (1-\tau)\theta'$, where $\tau \ll 1$ is a small constant.
    \noindent
    
    \item \textbf{Actor Update:} The actor is updated using the deterministic policy gradient theorem, which states that the gradient of the expected return with respect to the policy parameters can be computed as:
    \[
    \nabla_{\theta^\mu} J \approx \mathbb{E}_{s_t \sim D} [\nabla_a Q(s, a | \theta^Q)|_{s=s_t, a=\mu(s_t)} \nabla_{\theta^\mu} \mu(s_t | \theta^\mu)]
    \]
    \noindent
    This gradient ascent step improves the policy by moving it in the direction that increases the Q-value according to the critic's estimate.
    \noindent
    
    \item \textbf{Prioritized Experience Replay (PER):} This work enhances DDPG with PER, which addresses a fundamental limitation of uniform experience replay. Instead of sampling transitions uniformly from the replay buffer $D$, PER samples transitions according to their temporal-difference (TD) error, prioritizing experiences where the model's predictions are most inaccurate and therefore most informative. The probability of sampling transition $i$ is:
    \[
    P(i) = \frac{p_i^\beta}{\sum_k p_k^\beta}
    \]
    \noindent
    where $p_i = |\delta_i| + \epsilon$ is the priority based on the TD-error $\delta_i = y_t - Q(s_t, a_t)$, $\epsilon$ is a small constant to ensure non-zero probabilities, and $\beta$ controls the degree of prioritization. To correct for the bias introduced by non-uniform sampling, PER applies importance-sampling (IS) weights during the gradient update:
    \[
    w_i = \left(\frac{1}{N \cdot P(i)}\right)^\alpha
    \]
    \noindent
    where $\alpha$ controls the amount of bias correction. These weights ensure that the gradient estimator remains unbiased despite the non-uniform sampling distribution.
    \noindent
\end{itemize}

\paragraph{Implementation Details}
To integrate Prioritized Experience Replay (PER) with DDPG, a custom class, \texttt{CustomDDPG}, was developed in the \\ \noindent \texttt{ev2gym/rl\_agent/custom\_algorithms.py} module. This class inherits from the standard \texttt{DDPG} agent provided by \textbf{Stable-Baselines3}. The core \texttt{train} method is overridden to replace the default uniform-sampling replay buffer with one that supports prioritized sampling. This involves calculating TD-errors for each transition, updating their priorities in the buffer, and using the resulting importance-sampling weights during the critic update, thereby focusing the learning process on the most informative experiences.
\noindent

\subsubsection{Truncated Quantile Critics (TQC)}
TQC represents a significant advancement over standard SAC by modeling the entire distribution of returns rather than just its expected value. This distributional perspective, combined with a novel truncation mechanism, provides superior protection against Q-value overestimationâ€”a persistent pathology in off-policy RL that can lead to catastrophic policy degradation.
\noindent

\begin{itemize}
    \item \textbf{Distributional Learning:} TQC employs an ensemble of $N$ critic networks, \{$Q_{\phi_i}(s, a)\}_{i=1}^{N}$, where each network is trained to estimate a specific quantile $\tau_i$ of the return distribution. The target quantiles are implicitly defined as $\tau_i = \frac{i-0.5}{N}$, uniformly covering the cumulative distribution function. The critics are trained by minimizing the quantile Huber loss, $L_{QH}$, which is a robust asymmetric loss function that penalizes overestimation and underestimation differently:
    \[
    L_{QH}(\delta) = |\tau_i - \mathbb{I}_{\{\delta < 0\}}| \cdot L_\kappa(\delta)
    \]
    \noindent
    where $L_\kappa$ is the Huber loss with threshold $\kappa$, and $\delta$ is the temporal difference error.
    \noindent
    
    \item \textbf{Distributional Target Calculation:} A distributional target is constructed for the Bellman update. First, an action is sampled from the target policy for the next state: $\tilde{a}_{t+1} \sim \pi_{\theta'}(\cdot|s_{t+1})$. Then, a set of $N$ Q-value estimates for the next state is obtained from the $N$ target critic networks: \{$Q_{\phi'_j}(s_{t+1}, \tilde{a}_{t+1})\}_{j=1}^{N}$. This ensemble provides a rich representation of the value distribution's uncertainty.
    \noindent
    
    \item \textbf{Truncation:} This is the central innovation of TQC. To combat overestimation, the algorithm discards the $k$ largest Q-value estimates from the set of $N$ target values. This truncation removes the most optimistic (and typically most biased) estimates, which are the primary drivers of overestimation in ensemble methods. By dropping these outliers, TQC obtains more conservative and stable target values. The remaining $(N-k)$ quantile estimates are then used to compute the Bellman target, effectively implementing a form of pessimism that counteracts the inherent optimism bias in temporal-difference learning.
    \noindent
    
    \item \textbf{Critic Update:} The target value for updating the $i$-th critic is formed using the Bellman equation with the truncated set of next-state Q-values. The overall critic loss is the sum of the quantile losses across all critics:
    \[
    L(\phi) = \sum_{i=1}^{N} \mathbb{E}_{(s,a,r,s') \sim D} \left[ L_{QH}\left(r + \gamma Q_{\text{trunc}}(s', \tilde{a}') - Q_{\phi_i}(s,a) \right) \right]
    \]
    \noindent
    where $Q_{\text{trunc}}$ represents the value derived from the truncated set of target quantiles. The actor is then updated using the mean of all (non-truncated) critic estimates, similar to standard SAC, but benefiting from the improved stability of the distributional value estimates.
    \noindent
\end{itemize}

\paragraph{Implementation Details}
The TQC algorithm is leveraged from the \textbf{SB3-Contrib} library, a collection of community-contributed extensions to Stable-Baselines3. Using the library's standard \texttt{TQC} implementation allows the framework to benefit from this state-of-the-art distributional RL algorithm without requiring a custom implementation from scratch. The implementation maintains the entropy-regularized objective of SAC while incorporating the distributional critics and truncation mechanism that define TQC's superior performance characteristics.
\noindent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{A History-Based Adaptive Reward for Profit Maximization}
\label{sec:adaptive_reward}

\paragraph{Implementation Details}
This reward function, along with a suite of other reward strategies, is implemented in the
\\
\noindent
 \texttt{ev2gym/rl\_agent/reward.py} module. The main experimentation script, \texttt{run\_experiments.py}, allows the user to dynamically select which reward function to use for a given training run. The \texttt{FastProfitAdaptiveReward} function, specifically, uses \texttt{collections.deque} objects attached to the environment instance to efficiently maintain a sliding window of recent performance for satisfaction and overload events.
\noindent
To steer the learning agent towards policies that are both highly profitable and operationally reliable, we have designed a novel, history-based adaptive reward function named \texttt{FastProfitAdaptiveReward}. This function challenges traditional static-weight penalty approaches by introducing a dynamic feedback mechanism where penalty severity responds directly to the agent's recent performance. The underlying principle is straightforward but powerful: prioritize economic profit aggressively, while employing adaptive penalties as guardrails that tighten only when the agent begins to systematically violate operational constraints.
\noindent
The total reward at each timestep $t$, $R_t$, is calculated as the net economic profit minus any active penalties for user dissatisfaction or transformer overload.
\noindent

\begin{equation}
    R_t = \Pi_t - P_t^{\text{sat}} - P_t^{\text{tr}}
\end{equation}

\subsubsection{Economic Profit}
The foundation of the reward signal is the direct, instantaneous economic profit, $\Pi_t$. This component provides an unambiguous incentive for the agent to exploit market dynamics, encouraging charging during low-price periods and discharging (V2G) during high-price periods.
\noindent

\begin{equation}
    \Pi_t = \sum_{i=1}^{N} \left( C_t^{\text{sell}} \cdot P_{i,t}^{\text{dis}} - C_t^{\text{buy}} \cdot P_{i,t}^{\text{ch}} \right) \Delta t
\end{equation}
\noindent
where $N$ is the number of connected EVs, $C_t^{\text{sell}}$ and $C_t^{\text{buy}}$ are the electricity prices, and $P_{i,t}^{\text{dis}}$ and $P_{i,t}^{\text{ch}}$ are the discharging and charging powers for EV $i$.
\noindent

\subsubsection{Adaptive User Satisfaction Penalty}
The penalty for failing to meet user charging demands, $P_t^{\text{sat}}$, departs from fixed-value approaches. It adapts based on the system's recent performance history. The environment maintains a short-term memory of the average user satisfaction over the last 100 timesteps, from which we calculate an average satisfaction score, $\bar{S}_{hist}$.
\noindent
A \textit{satisfaction severity multiplier}, $\lambda_t^{\text{sat}}$, is then calculated. This multiplier grows quadratically as the historical average satisfaction drops. When the system has been performing poorly, the consequences for a new failure become substantially more severe.
\noindent
\begin{equation}
    \lambda_t^{\text{sat}} = \lambda_{\text{base}}^{\text{sat}} \cdot (1 - \bar{S}_{hist})^2
\end{equation}
\noindent
where $\lambda_{\text{base}}^{\text{sat}}$ is a base scaling factor (e.g., 20.0). A penalty is only applied if any departing EV's satisfaction, $S_k$, falls below a critical threshold (e.g., 95\%). The magnitude of the penalty is the product of the adaptive multiplier and the current satisfaction deficit.
\noindentW
\begin{equation}
    P_t^{\text{sat}} = \lambda_t^{\text{sat}} \cdot (1 - \min(S_k)) \quad \forall k \in \text{EVs departing at } t
\end{equation}
\noindent

This establishes a feedback loop with non-trivial consequences: a single, isolated failure in an otherwise well-performing system results in a mild penalty, whereas persistent failures trigger rapidly escalating penalties that force behavioral correction.
\noindent

\subsubsection{Adaptive Transformer Overload Penalty}
The transformer overload penalty, $P_t^{\text{tr}}$, operates on a similar adaptive principle, responding to the recent frequency of overloads. The environment tracks how often an overload has occurred in the last 100 timesteps, yielding an overload frequency, $F_{hist}^{\text{tr}}$.
\noindent

This frequency drives the computation of a linear \textit{overload severity multiplier}, $\lambda_t^{\text{tr}}$. Higher overload frequency translates directly to higher penalties for new violations.
\noindent

\begin{equation}
    \lambda_t^{\text{tr}} = \lambda_{\text{base}}^{\text{tr}} \cdot F_{hist}^{\text{tr}}
\end{equation}
\noindent
where $\lambda_{\text{base}}^{\text{tr}}$ is a base scaler (e.g., 50.0). If the total power drawn, $P_j^{\text{total}}(t)$, exceeds the transformer's limit, $P_j^{\text{max}}$, a penalty is applied. This penalty consists of a small, fixed base amount plus the adaptive component, which scales with the magnitude of the current overload.
\noindent

\begin{equation}
    P_t^{\text{tr}} = P_{\text{base}} + \lambda_t^{\text{tr}} \cdot \sum_{j=1}^{N_T} \max(0, P_j^{\text{total}}(t) - P_j^{\text{max}})
\end{equation}
\noindent

This mechanism enforces a critical lesson: while an occasional, minor overload might be tolerable in pursuit of high profit, habitual overloading becomes increasingly unsustainable as penalties escalate.
\noindent

\subsubsection{Rationale and Significance}
This history-based adaptive reward function represents a departure from static or purely state-based approaches. By making penalty weights a function of recent performance history, we provide a more sophisticated learning signal. The agent is not excessively punished for isolated, exploratory actions that might lead to minor constraint violations. Instead, it is strongly discouraged from developing policies that produce chronic system failures.
\noindent
The approach mirrors realistic management objectives: maintain high performance on average, and react decisively only when performance trends begin to deteriorate. This method is also computationally efficient, avoiding complex state-dependent calculations in favor of simple updates to historical data queues. The reward structure guides the agent to discover policies that balance economic objectives with operational reliability, achieving an intelligent equilibrium between profit ambition and system safety constraints.
\noindent