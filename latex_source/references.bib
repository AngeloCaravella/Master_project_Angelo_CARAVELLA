%% =====================================================================
%% A. Foundational Concepts & General Reviews
%% =====================================================================
%% Questa sezione include articoli di rassegna e lavori fondamentali che 
%% forniscono una panoramica completa del panorama Vehicle-to-Grid (V2G),
%% del ruolo dell'intelligenza artificiale e delle sfide di integrazione 
%% con le smart grid.
%% =====================================================================

@article{Xie2025,
  title={Reinforcement learning for vehicle-to-grid: A review},
  author={Xie, H.},
  journal={ScienceDirect},
  year={2025},
  doi={10.1016/j.sciad.2025.100008}
}

@article{Qiu2023,
  title={Reinforcement learning for electric vehicle applications in energy management: A review},
  author={Dawei Qiu and Yi Wang and Weiqi Hua and Goran Strbac},
  journal={Renewable and Sustainable Energy Reviews},
  volume={163},
  pages={112443},
  year={2023},
  doi={10.1016/j.rser.2022.112443}
}

@article{Zhang2023,
  title={Transfer deep reinforcement learning-based large-scale V2G continuous charging coordination with renewable energy sources},
  author={Yubao Zhang and Xin Chen and Yuchen Zhang},
  journal={arXiv preprint arXiv:2210.07013},
  year={2023}
}

@article{NaXu,
  title={A Review of Smart Grid Evolution and Reinforcement Learning: Applications, Challenges and Future Directions},
  author={Na Xu and Zhuo Tang and Chenyi Si and Jinshan Bian and Chaoxu Mu},
  journal={Energies},
  volume={18},
  number={7},
  pages={1837},
  year={2024},
  publisher={MDPI},
  doi={10.3390/en18071837}
}

@article{Feyijimi,
  title={A Review of Bidirectional Charging Grid Support Applications and Control},
  author={Feyijimi Adegbohun and Annette von Jouanne and Emmanuel Agamloh and Alex Yokochi},
  journal={Energies},
  volume={17},
  number={6},
  pages={1320},
  year={2024},
  publisher={MDPI},
  doi={10.3390/en17061320}
}

@article{Srihari,
  title={Integration of electric vehicle into smart grid: a meta heuristic algorithm for energy management between V2G and G2V},
  author={G. Srihari and R. S. R. Krishnam Naidu and Przemysław Falkowski-Gilski and Parameshachari Bidare Divakarachari and Ravi Kiran Varma Penmatsa},
  journal={Frontiers in Energy Research},
  volume={12},
  pages={1357863},
  year={2024},
  publisher={Frontiers Media SA},
  doi={10.3389/fenrg.2024.1357863}
}

@article{white2011vehicle,
  title={Using vehicle-to-grid technology for frequency regulation and peak-load
reduction},
  author={Corey D. White and K. Max Zhang},
  journal={Journal of Power Sources},
  volume={196},
  number={3},
  pages={3972--3980},
  year={2011},
  publisher={Elsevier},
  doi={10.1016/j.jpowsour.2010.11.010}
}

@article{salvatti2020electric,
  author={Gabriel Antonio Salvatti and Emerson Giovani Carati and Rafael Cardoso and Jean Patric da Costa and Carlos Marcelo de Oliveira Stein},
  title={Electric Vehicles Energy Management with V2G/G2V Multifactor Optimization of Smart Grids},
  journal={Energies},
  volume={13},
  number={5},
  pages={1191},
  year={2020},
  publisher={MDPI},
  doi={10.3390/en13051191}
}

@article{sadeghi2022deep,
  title={Cost and power loss aware coalitions under uncertainty in transactive energy systems},
  author={Mohammad Sadeghi},
  journal={Université d'Ottawa / University of Ottawa},
  year={2022},
  note={PhD Thesis}
}

@article{Tavakoli2019,
  author = {Ahmad Tavakoli and Sajeeb Saha and Mohammad Taufiqul Arif and Md Enamul Haque and Nishad Mendis and Aman M.T. Oo},
  
  title = {Impacts of grid integration of solar PV and electric vehicle on grid stability, power quality and energy economics: a review},
  journal = {IET Energy Systems Integration},
  volume = {2},
  number = {3},
  pages = {233--245},
  year = {2020},
  doi = {10.1049/iet-esi.2019.0047}
}


%% =====================================================================
%% B. Core Methodologies: Reinforcement Learning in V2G
%% =====================================================================
%% Questa sezione contiene articoli che applicano direttamente il Deep 
%% Reinforcement Learning (DRL) al problema della gestione della ricarica 
%% dei veicoli elettrici e dei servizi V2G.
%% =====================================================================
@article{krener2016adaptive,
  title={Adaptive Horizon Model Predictive Control},
  author={Krener, Arthur J},
  journal={arXiv preprint arXiv:1602.08619},
  year={2016},
  eprint={1602.08619},
  archivePrefix={arXiv},
  primaryClass={math.OC}
}


@article{alfaverh2022optima,
  title={Optimal vehicle-to-grid control for supplementary frequency regulation using deep reinforcement learning},
  author={Fayiz Alfaverh and Mouloud Denai and Yichuang Sun},
  journal={Applied Energy},
  volume={325},
  pages={119881},
  year={2022},
  publisher={Elsevier},
  doi={10.1016/j.apenergy.2022.119881}
}

@article{Dou,
  title={Optimal scheduling for charging and discharging of electric vehicles based on deep reinforcement learning},
  author={Dou An and Feifei Cui and Xun Kang},
  journal={Frontiers in Energy Research},
  volume={11},
  pages={1273820},
  year={2023},
  publisher={Frontiers},
  doi={10.3389/fenrg.2023.1273820}
}

@article{Ding,
  title={Deep reinforcement learning for charging scheduling of electric vehicles considering distribution network voltage stability},
  author={Ding Liu and Peng Zeng and Shijie Cui and Chunhe Song},
  journal={Sensors},
  volume={23},
  number={3},
  pages={1618},
  year={2023},
  publisher={MDPI},
  doi={10.3390/s23031618}
}


@article{chifu2024deep,
  title={A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in Smart Grids},
  author={Chifu, Viorica Rozina and Cioara, Tudor and Pop, Cristina Bianca and Rusu, Horia and Anghel, Ionu{\c{t}}},
  journal={arXiv preprint arXiv:2401.02653},
  year={2024},
  doi={10.48550/arXiv.2401.02653}
}


@article{wang2022electrical,
  title={An electrical vehicle-assisted demand response management system: A reinforcement learning method},
  author={Wang, Zhaoyu and Wang, Li and Wang, Siyuan and Zhang, Zhaoyang},
  journal={Frontiers in Energy Research},
  volume={10},
  pages={1071948},
  year={2022},
  publisher={Frontiers},
  doi={10.3389/fenrg.2022.1071948}
}




@article{karg2020efficient,
  title={Efficient representation and approximation of model predictive control laws via deep learning},
  author={Benjamin Karg and Sergio Lucia},
  journal={IEEE Transactions on Cybernetics},
  volume={50},
  number={9},
  pages={3866--3878},
  year={2020},
  doi={10.1109/TCYB.2020.2999556}
}



%% =====================================================================
%% C. Alternative Control Paradigms: Model Predictive Control (MPC)
%% =====================================================================
%% Questa sezione raggruppa gli articoli incentrati sul Model Predictive 
%% Control (MPC), il principale paradigma di controllo basato su modello 
%% utilizzato come benchmark per il confronto con gli agenti DRL.
%% =====================================================================
@book{camacho2013model,
  title={Model Predictive Control},
  author={E.F. Camacho and C. Bordons},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@article{Bemporad_Filippi,
author = {Bemporad, Alberto and Filippi, Carlo},
journal = {Proceedings of the 40th IEEE Conference on Decision and Control},
title = {Suboptimal Explicit MPC via Approximate Multiparametric Quadratic Programming},
year = {2001},
pages = {4851-4856},
month = {December},
}

@book{borrelli2017predictive,
  title={Predictive Control for Linear and Hybrid Systems},
  author={Borrelli, Francesco and Bemporad, Alberto and Morari, Manfred},
  year={2017},
  publisher={Cambridge University Press}
}

@article{bemporad2013explicit,
  title={Explicit Model Predictive Control},
  author={Bemporad, Alberto},
  journal={Springer Handbook of Automation},
  pages={883--898},
  year={2013},
  publisher={Springer}
}

@article{parisio2014mpc,
  title={A Model Predictive Control Approach to Microgrid Operation Optimization},
  author={Parisio, Alessandra and Rikos, Evangelos and Glielmo, Luigi},
  journal={IEEE Transactions on Control Systems Technology},
  volume={22},
  number={5},
  pages={1813--1827},
  year={2014},
  publisher={IEEE},
  doi={10.1109/TCST.2013.2295737}
}

@article{minchala2025systematic,
  title={A Systematic Review of Model Predictive Control for Robust and Efficient Energy Management in Electric Vehicle Integration and V2G Applications},
  author={Minchala-Ávila, Carlos A and Arévalo, Paúl and Ochoa-Correa, Diego},
  journal={Modelling},
  volume={6},
  number={1},
  pages={20},
  year={2025},
  publisher={MDPI},
  doi={10.3390/modelling6010020}
}

@mastersthesis{faggio2023design,
  title={Design and Testing of Online and Offline Optimization Algorithms for Vehicle-to-Grid (V2G) Industrial Applications},
  author={Gianluca Faggio},
  school={Politecnico di Milano},
  year={2023}
}



%% =====================================================================
%% D. Specific Technical Challenges & Models (Degradation, Simulation)
%% =====================================================================
%% Questa sezione contiene articoli che si concentrano su componenti 
%% cruciali del problema V2G, come la modellazione del degrado della 
%% batteria e i framework di simulazione utilizzati per la ricerca.
%% =====================================================================

@article{gu2025optimization,
  title={Optimization of Electric Vehicle Charging and Discharging Strategies Considering Battery Health State: A Safe Reinforcement Learning Approach},
  author={Shuifu Gu and Kejun Qian and Yongbiao Yang},
  journal={World Electric Vehicle Journal},
  volume={16},
  number={5},
  pages={286},
  year={2025},
  publisher={MDPI},
  doi={10.3390/wevj16050286}
}



@article{birkl2017degradation,
  title={Degradation diagnostics for lithium ion cells},
  author={Christoph R. Birkl and Matthew R. Roberts and Euan McTurk and Peter G. Bruce},
  journal={Journal of Power Sources},
  volume={341},
  pages={373--386},
  year={2017}
}
@mastersthesis{Bisetto_Francesca,
  author       = {Bisetto, Francesca},
  title        = {La Duck Curve e la scomposizione della stagionalità dei consumi elettrici. Aspetti teorici e modelli statistici},
  school       = {Università degli Studi di Padova},
  year         = {2023},
  address      = {Padova},
  type         = {Tesi di laurea magistrale},
  note         = {Dipartimento di Scienze Statistiche, Correlatore: Luigi Grossi}
}



@article{vetter2005ageing,
  title={Ageing mechanisms in lithium-ion batteries},
  author={Vetter, J{\"u}rgen and Nov{\'a}k, Petr and Wagner, Christoph and Veit, Christoph and M{\"o}ller, Klaus-Christian and Besenhard, J{\"u}rgen O. and Winter, Martin and Wohlfahrt-Mehrens, Margret and Vogler, Christoph and Hammouche, Asma},
  journal={Journal of Power Sources},
  volume={147},
  number={1-2},
  pages={269--281},
  year={2005},
  publisher={Elsevier}
}

@article{orfanoudakis2024ev2gym,
  title={EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking},
  author={Orfanoudakis, Stylianos and Diaz-Londono, Christian and Yilmaz, Yasin Emir and Palensky, Peter and Vergara, Pedro P.},
  journal={arXiv preprint arXiv:2404.01849},
  year={2024},
  doi={10.48550/arXiv.2404.01849}
}

@article{brockman2016openai,
  title={OpenAI Gym},
  author={Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016},
  doi={10.48550/arXiv.1606.01540}
}


%% =====================================================================
%% E. Foundational AI & Reinforcement Learning Theory
%% =====================================================================
%% Questa sezione include articoli seminali sugli algoritmi DRL e sui 
%% concetti teorici menzionati o utilizzati nella tesi, come le tecniche 
%% di reward shaping e gli algoritmi actor-critic.
%% =====================================================================

@article{Curriculum_learning,
  title   = {Curriculum Reinforcement Learning for Complex Reward Functions},
  author  = {Kilian Freitag and Kristian Ceder and Rita Laezza and Knut {\AA}kesson and Morteza Haghir Chehreghani},
  journal = {arXiv preprint arXiv:2410.16790},
  year    = {2024},
  institution = {Chalmers University of Technology, Gothenburg, Sweden; University of Gothenburg, Gothenburg, Sweden}
}

@article{Curriculum_learning_2,
  title   = {Comparing Reward Shaping, Visual Hints, and Curriculum Learning},
  author  = {Rey Pocius and David Isele and Mark Roberts and David W. Aha},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year    = {2019},
  address = {Corvallis, OR, USA; Philadelphia, PA, USA; Washington, DC, USA},
  institution = {Oregon State University; University of Pennsylvania; Naval Research Laboratory}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{fujimoto2018addressing,
  author    = {Scott Fujimoto and Herke van Hoof and David Meger},
  title     = {Addressing Function Approximation Error in Actor-Critic Methods},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
  year      = {2018},
  url       = {https://arxiv.org/abs/1802.09477}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{haarnoja2019soft,
  author    = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  title     = {Soft Actor-Critic Algorithms and Applications},
  journal   = {arXiv preprint arXiv:1812.05905},
  year      = {2019},
  url       = {https://arxiv.org/abs/1812.05905}
}

@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015},
  doi={10.48550/arXiv.1511.05952}
}

@article{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  journal={ University of California, Berkeley},
  year={1999}
}



@inproceedings{mnih2016asynchronous,
  author    = {Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  booktitle = {Proceedings of the 33rd International Conference on Machine Learning (ICML)},
  pages     = {1928--1937},
  year      = {2016},
  url       = {https://arxiv.org/abs/1602.01783}
}

@inproceedings{schulman2015trust,
  author    = {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
  title     = {Trust Region Policy Optimization},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
  pages     = {1889--1897},
  year      = {2015},
  url       = {https://arxiv.org/abs/1502.05477}
}

@inproceedings{schulman2017proximal,
  author    = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  pages     = {3371--3380},
  year      = {2017},
  url       = {https://arxiv.org/abs/1707.06347}
}

@inproceedings{kuznetsov2020controlling,
  author    = {Arsenii Kuznetsov and Pavel Shvechikov and Alexander Grishin and Dmitry Vetrov},
  title     = {Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
  year      = {2020},
  url       = {https://proceedings.mlr.press/v119/kuznetsov20a/kuznetsov20a.pdf}
}

@article{ibrahim2024comprehensive,
  author       = {Sinan Ibrahim and Mostafa Mostafa and Ali Jnadi and Hadi Salloum and Pavel Osinenko},
  title        = {Comprehensive Overview of Reward Engineering and Shaping in Advancing Reinforcement Learning Applications},
  journal      = {IEEE Access},
  volume       = {PP},
  number       = {99},
  pages        = {1--1},
  year         = {2024},
  doi          = {10.1109/ACCESS.2024.3504735},
  url          = {https://arxiv.org/abs/2408.10215}
}

@article{Fuente,
  author    = {Neil de la Fuente, Daniel A. Vidal Guerra},
  title     = {A Comparative Study of Deep Reinforcement Learning Algorithms},
  journal   = {arXiv preprint arXiv:2407.14151},
  year      = {2022},
  url       = {https://arxiv.org/abs/2407.14151}
}

@article{wang2022multi,
  author    = {Mingyu Wang and Yuxin Wen and Yifan Zhang and Zhiwei Luo and Zhiwei Steven Wu and Hongyu Wu and Hongyi Zhang},
  title     = {Multi-Agent Reinforcement Learning is a Sequence Modeling Problem},
  journal   = {arXiv preprint arXiv:2205.14953},
  year      = {2022},
  url       = {https://arxiv.org/abs/2205.14953}
}

@article{mania2018simple,
  author    = {Horia Mania and Aurelia Guy and Benjamin Recht},
  title     = {Simple Random Search Provides a Competitive Approach to Reinforcement Learning},
  journal   = {arXiv preprint arXiv:1803.07055},
  year      = {2018},
  url       = {https://arxiv.org/abs/1803.07055}
}


%% =====================================================================
%% F. Foundational Control Theory
%% =====================================================================
%% Questa sezione raccoglie i lavori classici e fondamentali sulla teoria 
%% del controllo che costituiscono la base teorica per approcci come il 
%% Model Predictive Control (MPC).
%% =====================================================================

@article{mayne2000constraine,
  title={Constrained model predictive control: Stability and optimality},
  author={D. Q. Mayne and J. B. Rawlings and C. V. Rao and P. O. M. Scokaert},
  journal={Automatica},
  volume={36},
  number={6},
  pages={789--814},
  year={2000},
  publisher={Elsevier},
  doi={10.1016/S0005-1098(99)00214-9}
}

@article{Richalet1978ModelPH,
  title={Model predictive heuristic control: Applications to industrial processes},
  author={Jacques Richalet and André Rault and Jean Louis Testud and Jean Papon},
  journal={Automatica},
  volume={14},
  number={5},
  pages={413--428},
  year={1978},
  publisher={Elsevier},
  doi={10.1016/0005-1098(78)90001-8}
}
@article{schulman2020trust,
  title={Trust Region Policy Optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  journal={Proceedings of the 32nd International Conference on Machine Learning (ICML},
  volume={37},
  pages={1889--1897},
  year={2015},
  publisher={PMLR},
  url={https://proceedings.mlr.press/v37/schulman15.html}
}
@inproceedings{Cutler1980,
  author    = {Cutler, C. R. and Ramaker, B. L.},
  title     = {Dynamic Matrix Control -- A Computer Control Algorithm},
  booktitle = {Proceedings of the Joint Automatic Control Conference},
  year      = {1980},
  address   = {San Francisco, USA},
  organization={American Control Conference},
  note      = {Paper No. WP5-B}
}

@article{Qiu2022,
  author    = {Qiu, Dajun and Wang, Siyuan and Wang, Beibei and Zhang, Nu and Li, Zhengmao},
  title     = {Reinforcement learning for vehicle-to-grid: A review},
  journal   = {Renewable and Sustainable Energy Reviews},
  volume    = {167},
  pages     = {112702},
  year      = {2022},
  publisher = {Elsevier}
}
@book{Sutton2018,
  title     = {Reinforcement Learning: An Introduction},
  edition   = {Second edition, in progress},
  author    = {Richard S. Sutton and Andrew G. Barto},
  year      = {2015},
  publisher = {The MIT Press},
  address   = {Cambridge, Massachusetts; London, England},
  note      = {A Bradford Book, c. 2014, 2015}
}
@article{NielsonElton2021InductionPopperML,
  title        = {Induction, Popper, and Machine Learning},
  author       = {Bruce Nielson and Daniel C. Elton},
  journal      = {arXiv preprint},
  volume       = {arXiv:2110.00840},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.00840},
  eprint       = {2110.00840},
  note         = {cs.AI}
}
@article{Mnih2015,
  author    = {Volodymyr Mnih, Koray Kavukcuoglu, David Silver et. al},
  title     = {Human-level control through deep reinforcement learning},
  journal   = {Nature},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015},
  publisher = {Nature Publishing Group}
}
%% =====================================================================
%% G. Policy, Regulation, and User-Centric Perspectives
%% =====================================================================
%% Questa sezione fornisce il contesto più ampio per il problema V2G, 
%% includendo documenti politici europei e ricerche incentrate sulla 
%% prospettiva dell'utente finale.
%% =====================================================================

@misc{european_commission_2021_fit_for_55,
  author       = {European Commission},
  title        = {Fit for 55: Delivering the EU's 2030 Climate Target},
  year         = {2021},
  url          = {https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX%3A52021DC0550}
}

@article{evertsson2024investigating,
  author       = {Li, Haijie and Asbj{\"o}rnsson, Gauti and Bhadani, Kanishk and Evertsson, Magnus},
  title        = {Investigating Dynamic Behavior in SAG Mill Pebble Recycling Circuits: A Simulation Approach},
  journal      = {Minerals},
  volume       = {14},
  number       = {7},
  pages        = {716},
  year         = {2024},
  doi          = {10.3390/min14070716},
  url          = {https://www.mdpi.com/2075-163X/14/7/716}
}