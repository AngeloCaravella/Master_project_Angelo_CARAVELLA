\section{Online MPC Formulation (PuLP Implementation)}

The Model Predictive Control (MPC) implemented with PuLP solves a profit maximization problem at each control interval over a finite prediction horizon $H$. This formulation is designed for online, real-time control, where decisions are made based on the current system state and future predictions. This approach is often termed an "implicit" MPC because the control law is not pre-computed; instead, it is found implicitly by solving a full optimization problem at every control interval. The logic is formally detailed in Algorithm \ref{alg:milp_mpc}.

\paragraph{Implementation Details}
This online controller is implemented as the \texttt{OnlineMPC\_Solver} class within the \texttt{ev2gym/baselines/pulp\_mpc.py} module. At each invocation of its \texttt{get\_action} method, it dynamically constructs the full Mixed-Integer Linear Program (MILP) described below using the \textbf{PuLP} modeling library. PuLP acts as a high-level modeling interface, which then calls an underlying solver. The problem is then solved using the default CBC (COIN-OR Branch and Cut) solver, an open-source solver capable of handling MILPs.

\begin{algorithm}[H]
\caption{Online Model Predictive Control (MILP)}
\label{alg:milp_mpc}
\begin{algorithmic}[1]
\Function{GetAction}{$E_{\text{initial}}, k_{\text{dep}}, c_{\text{buy}}, c_{\text{sell}}, H, N_c$}
    \State \textbf{Inputs:} Current energy states $E_{\text{initial}}$, departure times $k_{\text{dep}}$, price vectors $c_{\text{buy}}, c_{\text{sell}}$, prediction horizon $H$, control horizon $N_c$.
    
    \State \textbf{Initialize Problem:} Create a new MILP problem for profit maximization.
    
    \State \textbf{Define Variables} for $i \in \text{CS}, k \in [0, H-1]$:
    \State $P^{\text{ch}}_{i,k}, P^{\text{dis}}_{i,k} \in \mathbb{R}^+$ \Comment{Continuous power variables}
    \State $E_{i,k} \in \mathbb{R}^+$ \Comment{Continuous energy state variables}
    \State $z_{i,k} \in \{0, 1\}$ \Comment{Binary charge/discharge mode variables}
    
    \State \textbf{Define Objective Function:}
    \State Profit $\leftarrow \sum_{k=0}^{H-1} \sum_{i \in \text{CS}} \left[ (c^{\text{user}} - c^{\text{buy}}_k - c^{\text{deg}}) P^{\text{ch}}_{i,k} + (c^{\text{sell}}_k - c^{\text{deg}}) P^{\text{dis}}_{i,k} \right] \Delta t$
    \State Set objective to $\max(\text{Profit})$.
    
    \State \textbf{Add Constraints} for $i \in \text{CS}, k \in [0, H-1]$:
    \If{$k=0$}
        \State $E_{i,k} = E_{\text{initial},i} + (\eta_{\text{ch}} P^{\text{ch}}_{i,k} - \frac{1}{\eta_{\text{dis}}} P^{\text{dis}}_{i,k}) \cdot \Delta t$
    \Else
        \State $E_{i,k} = E_{i,k-1} + (\eta_{\text{ch}} P^{\text{ch}}_{i,k} - \frac{1}{\eta_{\text{dis}}} P^{\text{dis}}_{i,k}) \cdot \Delta t$
    \EndIf
    \State $0 \le P^{\text{ch}}_{i,k} \le P^{\text{ch,max}}_{i} \cdot z_{i,k}$
    \State $0 \le P^{\text{dis}}_{i,k} \le P^{\text{dis,max}}_{i} \cdot (1 - z_{i,k})$
    \State $E^{\text{min}}_{i} \le E_{i,k} \le E^{\text{max}}_{i}$
    \If{$k = k_{\text{dep},i}$}
        \State $E_{i,k} \ge E^{\text{des}}_{i}$
    \EndIf
    \State $\sum_{i \in \text{CS}} (P^{\text{ch}}_{i,k} - P^{\text{dis}}_{i,k}) \le P^{\text{tr,max}}$
    
    \State \textbf{Solve Problem:}
    \State solution $\leftarrow$ \Call{SolveMILP}{Problem}
    
    \If{solution is Optimal}
        \State Extract action plan $\{a_k^*\}_{k=0}^{N_c-1}$ from solution.
        \State \Return action plan
    \Else
        \State \Return empty plan or default action
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Mathematical Formulation}
The mathematical structure of the optimization problem solved in Algorithm \ref{alg:milp_mpc} is a classic \textbf{Mixed-Integer Linear Program (MILP)}. This classification is justified as follows:
\begin{itemize}
    \item \textbf{Linear Objective Function:} The objective function is a linear combination of the continuous power variables $P^{\text{ch}}$ and $P^{\text{dis}}$.
    \item \textbf{Linear Constraints:} All system constraints, including energy dynamics, power limits, and state of energy bounds, are formulated as linear equations or inequalities.
    \item \textbf{Mixed-Integer Variables:} The formulation employs both continuous variables (e.g., $P^{\text{ch}}_{i,k}$, $E_{i,k}$) and discrete, binary integer variables ($z_{i,k}$). The binary variables are essential for modeling the logical decision to either charge or discharge at any given time step.
\end{itemize}

\section{Quadratic MPC Formulation (CVXPY Implementation)}

As an alternative to the purely linear model, a quadratic formulation is also implemented. This version extends the previous MILP by introducing quadratic penalty terms into the objective function. While it shares the same underlying constraint structure, the change in the objective function alters the problem's nature to a \textbf{Mixed-Integer Quadratic Program (MIQP)}. The control logic is detailed in Algorithm \ref{alg:miqp_mpc}.

\paragraph{Implementation Details}
This controller is built using \textbf{CVXPY}, a Python-embedded modeling language for convex optimization problems. Due to the presence of both quadratic terms and integer variables, this formulation requires a solver capable of handling MIQPs, such as SCIP, Gurobi, or MOSEK.

\begin{algorithm}[H]
\caption{Online Model Predictive Control (MIQP)}
\label{alg:miqp_mpc}
\begin{algorithmic}[1]
\Function{GetAction}{$E_{\text{initial}}, k_{\text{dep}}, c_{\text{buy}}, c_{\text{sell}}, H, N_c$}
    \State \textbf{Inputs:} Same as Algorithm \ref{alg:milp_mpc}.
    
    \State \textbf{Initialize Problem:} Create a new MIQP problem.
    
    \State \textbf{Define Variables:} Same as Algorithm \ref{alg:milp_mpc}.
    
    \State \textbf{Define Objective Function:}
    \State LinearProfit $\leftarrow \sum_{k=0}^{H-1} \sum_{i \in \text{CS}} \left[ (c^{\text{user}} - c^{\text{buy}}_k - c^{\text{deg}}) P^{\text{ch}}_{i,k} + (c^{\text{sell}}_k - c^{\text{deg}}) P^{\text{dis}}_{i,k} \right] \Delta t$
    \State QuadraticPenalty $\leftarrow \sum_{k=0}^{H-1} \sum_{i \in \text{CS}} \left[ \lambda_{\text{ch}} (P^{\text{ch}}_{i,k})^2 + \lambda_{\text{dis}} (P^{\text{dis}}_{i,k})^2 \right] \Delta t$
    \State Set objective to $\max(\text{LinearProfit} - \text{QuadraticPenalty})$.
    
    \State \textbf{Add Constraints:} Same as Algorithm \ref{alg:milp_mpc}.
    
    \State \textbf{Solve Problem:}
    \State solution $\leftarrow$ \Call{SolveMIQP}{Problem}
    
    \If{solution is Optimal}
        \State Extract action plan $\{a_k^*\}_{k=0}^{N_c-1}$ from solution.
        \State \Return action plan
    \Else
        \State \Return empty plan or default action
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

The motivation for the quadratic penalty terms is to encourage smoother control actions. By making very high power flows quadratically more "expensive," the optimizer is incentivized to find solutions that are less aggressive, which can be beneficial for battery health and for reducing stress on the local grid infrastructure.

\section{Lyapunov-based Adaptive Horizon MPC}

A key enhancement developed in this work is the \textbf{Lyapunov-based Adaptive Horizon MPC}, which aims to reduce the computational burden of the online MPC while retaining its optimality and stability guarantees. The controller dynamically adjusts its prediction horizon $H_t$ based on the stability of the system, which is formally assessed using a Lyapunov function. A Lyapunov function $V(x)$ is a scalar function that measures the system's deviation from a desired equilibrium state. For the V2G system, we define it as the sum of squared errors from the desired final energy state:
\begin{equation}
    V(E_t) = \sum_{i \in \text{EVs}} (E_{i,t} - E_i^{\text{des}})^2
\end{equation}
The adaptive control logic, which wraps either the MILP or MIQP solver, is detailed in Algorithm \ref{alg:lyapunov_mpc}.

\begin{algorithm}[H]
\caption{Lyapunov-based Adaptive Horizon MPC}
\label{alg:lyapunov_mpc}
\begin{algorithmic}[1]
\State \textbf{Initialization:}
\State Initialize current horizon $H_{\text{current}} \leftarrow H_{\text{max}}$.
\State Define horizon bounds $H_{\text{min}}, H_{\text{max}}$ and convergence rate $\alpha$.
\State Initialize action plan $\mathcal{A} \leftarrow \emptyset$.

\Function{GetAdaptiveAction}{current state $s_t$}
    \If{action plan $\mathcal{A}$ is not empty}
        \State $a_t \leftarrow \text{Pop first action from } \mathcal{A}$.
        \State \Return $a_t$.
    \EndIf
    
    \State \Comment{Plan is empty, re-optimization is needed}
    \State $E_t \leftarrow \text{Get current energy states from } s_t$.
    \State $V(E_t) \leftarrow \sum_{i} (E_{i,t} - E_i^{\text{des}})^2$.
    
    \State \Comment{Solve MPC with the current horizon}
    \State solution $\leftarrow$ \Call{SolveMPC}{$s_t, H_{\text{current}}$} \Comment{Using MILP or MIQP solver}
    
    \If{solution is Optimal}
        \State Extract optimal first action $a_t^* = (P^{\text{ch,*}}_{t}, P^{\text{dis,*}}_{t})$.
        \State Predict next energy state $E_{t+1}$ using $a_t^*$.
        \State $V(E_{t+1}) \leftarrow \sum_{i} (E_{i,t+1} - E_i^{\text{des}})^2$.
        
        \State \Comment{Verify Lyapunov stability condition}
        \If{$V(E_{t+1}) \le V(E_t) - \alpha V(E_t)$}
            \State \Comment{Stable: reduce computational load for next cycle}
            \State $H_{\text{next}} \leftarrow \max(H_{\text{min}}, H_{\text{current}} - 1)$.
        \Else
            \State \Comment{Not stable enough: increase planning depth}
            \State $H_{\text{next}} \leftarrow \min(H_{\text{max}}, H_{\text{current}} + 1)$.
        \EndIf
        \State Extract new action plan $\mathcal{A}$ from solution.
    \Else
        \State \Comment{Solver failed: increase horizon as a safeguard}
        \State $H_{\text{next}} \leftarrow \min(H_{\text{max}}, H_{\text{current}} + 1)$.
        \State $\mathcal{A} \leftarrow \text{default safe action}$.
    \EndIf
    
    \State $H_{\text{current}} \leftarrow H_{\text{next}}$.
    \State $a_t \leftarrow \text{Pop first action from } \mathcal{A}$.
    \State \Return $a_t$.
\EndFunction
\end{algorithmic}
\end{algorithm}

This intelligent adjustment makes the online MPC more efficient and practical, reducing computation time during stable periods while retaining the ability to perform deep planning when necessary to guarantee system stability and constraint satisfaction.



%%%%%%%%%%%%%%%
\section{Approximate Explicit MPC: A Machine Learning Approach}
The online, implicit MPC formulation provides high-quality control decisions by solving an optimization problem at every time step. However, this approach has a significant drawback: its computational complexity. For scenarios with a large number of EVs or a long control horizon, solving a Mixed-Integer Linear Program (MILP) in real-time can be prohibitively slow, making it impractical for many real-world applications.
\noindent
To overcome this limitation, this work implements an \textbf{Approximate Explicit Model Predictive Controller (A-MPC)}. This controller leverages machine learning to replace the computationally expensive online optimization with a fast, lightweight inference step.

\subsection{Methodology: From Oracle to Apprentice}
The core idea is to treat the slow but powerful online MPC as an "oracle" or expert teacher. An apprentice model, $f_{\theta}$, is trained to mimic the oracle's behavior. The process involves an offline training phase to generate the model, followed by a fast online inference phase for real-time control. The state $s_t$ used for this mapping is a fixed-size vector summarizing all necessary information for a control decision:
\begin{equation}
    s_t = [ \mathbf{SoC}, \mathbf{T}^{\text{rem}}, \mathbf{C}^{\text{ch}}, \mathbf{C}^{\text{dis}} ]^T
\end{equation}
where $\mathbf{SoC}$ is the vector of current States of Charge, $\mathbf{T}^{\text{rem}}$ is the vector of remaining times until departure, and $\mathbf{C}^{\text{ch}}, \mathbf{C}^{\text{dis}}$ are the vectors of predicted future electricity prices over the horizon $H$. All vectors are padded to a maximum size to ensure a consistent input dimension for the model.

\subsection{Approximation via Random Forest}
The first implementation of the A-MPC uses a Random Forest, a powerful ensemble learning method known for its robustness and good performance on tabular data without extensive hyperparameter tuning.

\paragraph{Offline Training}
The training process, detailed in Algorithm \ref{alg:ampc_training_rf}, involves generating a large dataset by repeatedly querying the MPC oracle across many diverse scenarios and system states. A \texttt{RandomForestRegressor} model is then trained on this dataset using a standard fitting procedure.

\begin{algorithm}[H]
\caption{Offline Training of Random Forest MPC Approximator}
\label{alg:ampc_training_rf}
\begin{algorithmic}[1]
\State \textbf{Inputs:} Number of samples $N_{samples}$, Set of scenarios $\mathcal{S}$, MPC Oracle $O_{MPC}$.
\State \textbf{Initialize:} Empty datasets $X \leftarrow \emptyset$, $Y \leftarrow \emptyset$.

\For{$i = 1$ to $N_{samples}$} \Comment{Data Generation Loop}
    \State Randomly select a scenario $s_{conf} \in \mathcal{S}$ and initialize environment $env$.
    \State Select a random timestep $t_{rand}$ and advance $env$ to that state.
    \State Construct state vector $s_t \leftarrow$ \Call{BuildStateVector}{$env$}.
    \State Obtain optimal action from oracle: $a_t^* \leftarrow O_{MPC}(s_t)$.
    \If{$a_t^*$ is a valid, non-trivial action}
        \State Append $s_t$ to dataset $X$; Append $a_t^*$ to dataset $Y$.
    \EndIf
\EndFor

\State \Comment{Model Training}
\State Initialize model $f_{\theta} \leftarrow \text{RandomForestRegressor}(\text{hyperparameters})$.
\State Train the model on the entire dataset: $f_{\theta}.\text{fit}(X, Y)$.
\State \Return Trained model $f_{\theta}$.
\end{algorithmic}
\end{algorithm}

\paragraph{Implementation Details}
This controller is implemented in the \texttt{ApproximateExplicitMPC} class. The model is generated by the \texttt{train\_mpc\_approximator.py} script, which executes the steps outlined in Algorithm \ref{alg:ampc_training_rf}. The trained \textbf{scikit-learn} model is serialized to \texttt{mpc\_approximator.joblib}.

\subsection{Approximation via Deep ReLU Network}
While the Random Forest provides a powerful general-purpose approximation, a more theoretically grounded approach for this problem is to use a deep neural network with Rectified Linear Unit (ReLU) activation functions. As detailed in Chapter 2, the explicit solution to a linear MPC problem is a Piecewise Affine (PWA) function, and a deep ReLU network is theoretically capable of exactly representing such a function \cite{karg2020efficient}.

\paragraph{Offline Training}
The training methodology for the neural network, detailed in Algorithm \ref{alg:ampc_training_nn}, follows the same oracle-apprentice data generation paradigm. However, the training phase is iterative, involving epochs, mini-batches, and gradient-based optimization to minimize the Mean Squared Error (MSE) between the network's predictions and the oracle's actions.

\begin{algorithm}[H]
\caption{Offline Training of Neural Network MPC Approximator}
\label{alg:ampc_training_nn}
\begin{algorithmic}[1]
\State \textbf{Inputs:} $N_{samples}$, $\mathcal{S}$, $O_{MPC}$, epochs, batch size $B$, learning rate $\eta$.
\State \textbf{Data Generation:}
\State Generate datasets $(X, Y)$ using the same procedure as lines 2-10 in Algorithm \ref{alg:ampc_training_rf}.

\State \Comment{Model Training}
\State Initialize model $f_{\theta} \leftarrow \text{MPCApproximatorNet}(\text{architecture})$.
\State Initialize optimizer (e.g., Adam) with learning rate $\eta$.
\State Initialize loss function $L \leftarrow \text{MSELoss}$.
\State Create DataLoader $D_L$ from $(X, Y)$ with batch size $B$.

\For{epoch = 1 to epochs}
    \For{batch $(s_b, a_b)$ in $D_L$}
        \State Zero gradients: optimizer.zero\_grad().
        \State \Comment{Forward pass}
        \State Predict actions: $\hat{a}_b \leftarrow f_{\theta}(s_b)$.
        \State \Comment{Compute loss and backpropagate}
        \State loss $\leftarrow L(\hat{a}_b, a_b)$.
        \State loss.backward().
        \State \Comment{Update model weights}
        \State optimizer.step().
    \EndFor
\EndFor
\State \Return Trained model $f_{\theta}$.
\end{algorithmic}
\end{algorithm}

\paragraph{Implementation Details}
This controller is implemented as the \texttt{ApproximateExplicitMPC\_NN} class. The \texttt{train\_mpc\_approximator\_nn.py} script executes Algorithm \ref{alg:ampc_training_nn}, parallelizing the data generation process for efficiency. The trained \textbf{PyTorch} model is saved to \texttt{mpc\_approximator\_nn.pth}.

\subsection{Online Inference}
Once either the Random Forest or the Neural Network model is trained, it can be deployed for real-time control. The online inference process, shown in Algorithm \ref{alg:ampc_inference}, is identical for both approximators and is orders of magnitude faster than solving the online MPC problem.

\begin{algorithm}[H]
\caption{Online Inference with Trained A-MPC}
\label{alg:ampc_inference}
\begin{algorithmic}[1]
\State \textbf{Input:} A trained apprentice model $f_{\theta}$ (either RF or NN), current environment state $env_t$.
\Function{GetAction}{$env_t$}
    \State Construct state vector $s_t \leftarrow$ \Call{BuildStateVector}{$env_t$}.
    \State Compute action via fast inference: $a_t \leftarrow f_{\theta}(s_t)$.
    \State \Return $a_t$.
\EndFunction
\end{algorithmic}
\end{algorithm}