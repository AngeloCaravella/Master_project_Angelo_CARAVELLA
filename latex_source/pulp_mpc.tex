\section{Online MPC Formulation (PuLP Implementation)}

The Model Predictive Control (MPC) implemented with PuLP solves a profit maximization problem at each control interval over a finite prediction horizon $H$. This formulation is designed for online, real-time control, where decisions are made based on the current system state and future predictions. This approach is often termed an "implicit" MPC because the control law is not pre-computed; instead, it is found implicitly by solving a full optimization problem at every control interval. The logic is formally detailed in Algorithm \ref{alg:milp_mpc}.

\paragraph{Implementation Details}
This online controller is implemented as the \texttt{OnlineMPC\_Solver} class within the \texttt{ev2gym/baselines/pulp\_mpc.py} module. At each invocation of its \texttt{get\_action} method, it dynamically constructs the full Mixed-Integer Linear Program (MILP) described below using the \textbf{PuLP} modeling library. PuLP acts as a high-level modeling interface, which then calls an underlying solver. The problem is then solved using the default CBC (COIN-OR Branch and Cut) solver, an open-source solver capable of handling MILPs.

\begin{algorithm}[H]
\caption{Online Model Predictive Control (MILP)}
\label{alg:milp_mpc}
\begin{algorithmic}[1]
\Function{GetAction}{$E_{\text{initial}}, k_{\text{dep}}, c_{\text{buy}}, c_{\text{sell}}, H, N_c$}
    \State \textbf{Inputs:} Current energy states $E_{\text{initial}}$, departure times $k_{\text{dep}}$, price vectors $c_{\text{buy}}, c_{\text{sell}}$, prediction horizon $H$, control horizon $N_c$.
    
    \State \textbf{Initialize Problem:} Create a new MILP problem for profit maximization.
    
    \State \textbf{Define Variables} for $i \in \text{CS}, k \in [0, H-1]$:
    \State $P^{\text{ch}}_{i,k}, P^{\text{dis}}_{i,k} \in \mathbb{R}^+$ \Comment{Continuous power variables}
    \State $E_{i,k} \in \mathbb{R}^+$ \Comment{Continuous energy state variables}
    \State $z_{i,k} \in \{0, 1\}$ \Comment{Binary charge/discharge mode variables}
    
    \State \textbf{Define Objective Function:}
    \State Profit $\leftarrow \sum_{k=0}^{H-1} \sum_{i \in \text{CS}} \left[ (c^{\text{user}} - c^{\text{buy}}_k - c^{\text{deg}}) P^{\text{ch}}_{i,k} + (c^{\text{sell}}_k - c^{\text{deg}}) P^{\text{dis}}_{i,k} \right] \Delta t$
    \State Set objective to $\max(\text{Profit})$.
    
    \State \textbf{Add Constraints} for $i \in \text{CS}, k \in [0, H-1]$:
    \If{$k=0$}
        \State $E_{i,k} = E_{\text{initial},i} + (\eta_{\text{ch}} P^{\text{ch}}_{i,k} - \frac{1}{\eta_{\text{dis}}} P^{\text{dis}}_{i,k}) \cdot \Delta t$
    \Else
        \State $E_{i,k} = E_{i,k-1} + (\eta_{\text{ch}} P^{\text{ch}}_{i,k} - \frac{1}{\eta_{\text{dis}}} P^{\text{dis}}_{i,k}) \cdot \Delta t$
    \EndIf
    \State $0 \le P^{\text{ch}}_{i,k} \le P^{\text{ch,max}}_{i} \cdot z_{i,k}$
    \State $0 \le P^{\text{dis}}_{i,k} \le P^{\text{dis,max}}_{i} \cdot (1 - z_{i,k})$
    \State $E^{\text{min}}_{i} \le E_{i,k} \le E^{\text{max}}_{i}$
    \If{$k = k_{\text{dep},i}$}
        \State $E_{i,k} \ge E^{\text{des}}_{i}$
    \EndIf
    \State $\sum_{i \in \text{CS}} (P^{\text{ch}}_{i,k} - P^{\text{dis}}_{i,k}) \le P^{\text{tr,max}}$
    
    \State \textbf{Solve Problem:}
    \State solution $\leftarrow$ \Call{SolveMILP}{Problem}
    
    \If{solution is Optimal}
        \State Extract action plan $\{a_k^*\}_{k=0}^{N_c-1}$ from solution.
        \State \Return action plan
    \Else
        \State \Return empty plan or default action
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Mathematical Formulation}
The mathematical structure of the optimization problem solved in Algorithm \ref{alg:milp_mpc} is a classic \textbf{Mixed-Integer Linear Program (MILP)}. This classification is justified as follows:
\begin{itemize}
    \item \textbf{Linear Objective Function:} The objective function is a linear combination of the continuous power variables $P^{\text{ch}}$ and $P^{\text{dis}}$.
    \item \textbf{Linear Constraints:} All system constraints, including energy dynamics, power limits, and state of energy bounds, are formulated as linear equations or inequalities.
    \item \textbf{Mixed-Integer Variables:} The formulation employs both continuous variables (e.g., $P^{\text{ch}}_{i,k}$, $E_{i,k}$) and discrete, binary integer variables ($z_{i,k}$). The binary variables are essential for modeling the logical decision to either charge or discharge at any given time step.
\end{itemize}

\section{Quadratic MPC Formulation (CVXPY Implementation)}

As an alternative to the purely linear model, a quadratic formulation is also implemented. This version extends the previous MILP by introducing quadratic penalty terms into the objective function. While it shares the same underlying constraint structure, the change in the objective function alters the problem's nature to a \textbf{Mixed-Integer Quadratic Program (MIQP)}. The control logic is detailed in Algorithm \ref{alg:miqp_mpc}.

\paragraph{Implementation Details}
This controller is built using \textbf{CVXPY}, a Python-embedded modeling language for convex optimization problems. Due to the presence of both quadratic terms and integer variables, this formulation requires a solver capable of handling MIQPs, such as SCIP, Gurobi, or MOSEK.

\begin{algorithm}[H]
\caption{Online Model Predictive Control (MIQP)}
\label{alg:miqp_mpc}
\begin{algorithmic}[1]
\Function{GetAction}{$E_{\text{initial}}, k_{\text{dep}}, c_{\text{buy}}, c_{\text{sell}}, H, N_c$}
    \State \textbf{Inputs:} Same as Algorithm \ref{alg:milp_mpc}.
    
    \State \textbf{Initialize Problem:} Create a new MIQP problem.
    
    \State \textbf{Define Variables:} Same as Algorithm \ref{alg:milp_mpc}.
    
    \State \textbf{Define Objective Function:}
    \State LinearProfit $\leftarrow \sum_{k=0}^{H-1} \sum_{i \in \text{CS}} \left[ (c^{\text{user}} - c^{\text{buy}}_k - c^{\text{deg}}) P^{\text{ch}}_{i,k} + (c^{\text{sell}}_k - c^{\text{deg}}) P^{\text{dis}}_{i,k} \right] \Delta t$
    \State QuadraticPenalty $\leftarrow \sum_{k=0}^{H-1} \sum_{i \in \text{CS}} \left[ \lambda_{\text{ch}} (P^{\text{ch}}_{i,k})^2 + \lambda_{\text{dis}} (P^{\text{dis}}_{i,k})^2 \right] \Delta t$
    \State Set objective to $\max(\text{LinearProfit} - \text{QuadraticPenalty})$.
    
    \State \textbf{Add Constraints:} Same as Algorithm \ref{alg:milp_mpc}.
    
    \State \textbf{Solve Problem:}
    \State solution $\leftarrow$ \Call{SolveMIQP}{Problem}
    
    \If{solution is Optimal}
        \State Extract action plan $\{a_k^*\}_{k=0}^{N_c-1}$ from solution.
        \State \Return action plan
    \Else
        \State \Return empty plan or default action
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

The motivation for the quadratic penalty terms is to encourage smoother control actions. By making very high power flows quadratically more "expensive," the optimizer is incentivized to find solutions that are less aggressive, which can be beneficial for battery health and for reducing stress on the local grid infrastructure.

\section{Lyapunov-based Adaptive Horizon MPC}

A key enhancement developed in this work is the \textbf{Lyapunov-based Adaptive Horizon MPC}, which aims to reduce the computational burden of the online MPC while retaining its optimality and stability guarantees. The controller dynamically adjusts its prediction horizon $H_t$ based on the stability of the system, which is formally assessed using a Lyapunov function. A Lyapunov function $V(x)$ is a scalar function that measures the system's deviation from a desired equilibrium state. For the V2G system, we define it as the sum of squared errors from the desired final energy state:
\begin{equation}
    V(E_t) = \sum_{i \in \text{EVs}} (E_{i,t} - E_i^{\text{des}})^2
\end{equation}
The adaptive control logic, which wraps either the MILP or MIQP solver, is detailed in Algorithm \ref{alg:lyapunov_mpc}.

\begin{algorithm}[H]
\caption{Lyapunov-based Adaptive Horizon MPC}
\label{alg:lyapunov_mpc}
\begin{algorithmic}[1]
\State \textbf{Initialization:}
\State Initialize current horizon $H_{\text{current}} \leftarrow H_{\text{max}}$.
\State Define horizon bounds $H_{\text{min}}, H_{\text{max}}$ and convergence rate $\alpha$.
\State Initialize action plan $\mathcal{A} \leftarrow \emptyset$.

\Function{GetAdaptiveAction}{current state $s_t$}
    \If{action plan $\mathcal{A}$ is not empty}
        \State $a_t \leftarrow \text{Pop first action from } \mathcal{A}$.
        \State \Return $a_t$.
    \EndIf
    
    \State \Comment{Plan is empty, re-optimization is needed}
    \State $E_t \leftarrow \text{Get current energy states from } s_t$.
    \State $V(E_t) \leftarrow \sum_{i} (E_{i,t} - E_i^{\text{des}})^2$.
    
    \State \Comment{Solve MPC with the current horizon}
    \State solution $\leftarrow$ \Call{SolveMPC}{$s_t, H_{\text{current}}$} \Comment{Using MILP or MIQP solver}
    
    \If{solution is Optimal}
        \State Extract optimal first action $a_t^* = (P^{\text{ch,*}}_{t}, P^{\text{dis,*}}_{t})$.
        \State Predict next energy state $E_{t+1}$ using $a_t^*$.
        \State $V(E_{t+1}) \leftarrow \sum_{i} (E_{i,t+1} - E_i^{\text{des}})^2$.
        
        \State \Comment{Verify Lyapunov stability condition}
        \If{$V(E_{t+1}) \le V(E_t) - \alpha V(E_t)$}
            \State \Comment{Stable: reduce computational load for next cycle}
            \State $H_{\text{next}} \leftarrow \max(H_{\text{min}}, H_{\text{current}} - 1)$.
        \Else
            \State \Comment{Not stable enough: increase planning depth}
            \State $H_{\text{next}} \leftarrow \min(H_{\text{max}}, H_{\text{current}} + 1)$.
        \EndIf
        \State Extract new action plan $\mathcal{A}$ from solution.
    \Else
        \State \Comment{Solver failed: increase horizon as a safeguard}
        \State $H_{\text{next}} \leftarrow \min(H_{\text{max}}, H_{\text{current}} + 1)$.
        \State $\mathcal{A} \leftarrow \text{default safe action}$.
    \EndIf
    
    \State $H_{\text{current}} \leftarrow H_{\text{next}}$.
    \State $a_t \leftarrow \text{Pop first action from } \mathcal{A}$.
    \State \Return $a_t$.
\EndFunction
\end{algorithmic}
\end{algorithm}

This intelligent adjustment makes the online MPC more efficient and practical, reducing computation time during stable periods while retaining the ability to perform deep planning when necessary to guarantee system stability and constraint satisfaction.



%%%%%%%%%%%%%%%