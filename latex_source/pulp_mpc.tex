\section{Online MPC Formulation (PuLP Implementation)}

The Model Predictive Control (MPC) implemented with PuLP solves a profit maximization problem at each time step $t$ over a finite prediction horizon $H$. This formulation is designed for online, real-time control, where decisions are made based on the current system state and future predictions.

\paragraph{Implementation Details}
This online controller is implemented as the \texttt{OnlineMPC\_Solver} class within the \texttt{ev2gym/baselines/pulp\_mpc.py} module. At each invocation of its \texttt{get\_action} method, it dynamically constructs the full Mixed-Integer Linear Program (MILP) described below using the \textbf{PuLP} modeling library. The problem is then solved using the default CBC (COIN-OR Branch and Cut) solver.

\subsection{Mathematical Formulation}
At each time step $t$, the MPC controller solves the following optimization problem.

\subsubsection{Objective Function: Net Operational Profit}
The objective is to maximize the total net operational profit over the control horizon $H$. This provides a comprehensive economic model that goes beyond simple energy arbitrage.
\begin{equation}
\max_{P^{\text{ch}}, P^{\text{dis}}, z} \sum_{k=t}^{t+H-1} \sum_{i \in \text{CS}} \left( \text{Revenues}_{i,k} - \text{Costs}_{i,k} \right)
\end{equation}
The revenue and cost components are defined for each station $i$ at time step $k$ as:
\begin{itemize}
    \item \textbf{Revenues} consist of:
    \begin{itemize}
        \item Grid Sales Revenue (V2G): $c^{\text{sell}}_k \cdot P^{\text{dis}}_{i,k} \cdot \Delta t$
        \item User Charging Revenue: $c^{\text{user}} \cdot P^{\text{ch}}_{i,k} \cdot \Delta t$
    \end{itemize}
    \item \textbf{Costs} consist of:
    \begin{itemize}
        \item Grid Purchase Cost: $c^{\text{buy}}_k \cdot P^{\text{ch}}_{i,k} \cdot \Delta t$
        \item Battery Degradation Cost: $c^{\text{deg}} \cdot (P^{\text{ch}}_{i,k} + P^{\text{dis}}_{i,k}) \cdot \Delta t$
    \end{itemize}
\end{itemize}
where $c^{\text{sell}}_k$ and $c^{\text{buy}}_k$ are the time-varying electricity prices, $c^{\text{user}}$ is the fixed price for the end-user, $c^{\text{deg}}$ is the estimated cost of battery degradation per kWh cycled, and $\Delta t$ is the time step duration.

\subsubsection{System Constraints}
The optimization is subject to the following constraints for each station $i$ and time step $k \in [t, t+H-1]$.

\paragraph{Energy Balance Dynamics.} The state of energy of the EV battery evolves according to:
\begin{equation}
E_{i,k} = E_{i,k-1} + \left( \eta_{\text{ch}} P^{\text{ch}}_{i,k} - \frac{1}{\eta_{\text{dis}}} P^{\text{dis}}_{i,k} \right) \cdot \Delta t
\end{equation}
where the initial state $E_{i,t-1}$ is the currently measured energy level of the EV.

\paragraph{Power Limits and Mutual Exclusion.} Charging and discharging powers are bounded by the EV's capabilities and controlled by a binary variable $z_{i,k}$ to prevent simultaneous operation.
\begin{align}
    0 &\le P^{\text{ch}}_{i,k} \le P^{\text{ch,max}}_{i} \cdot z_{i,k} \\
    0 &\le P^{\text{dis}}_{i,k} \le P^{\text{dis,max}}_{i} \cdot (1 - z_{i,k})
\end{align}

\paragraph{State of Energy (SoE) Limits.} The battery energy level must remain within its physical operational window.
\begin{equation}
E^{\text{min}}_{i} \le E_{i,k} \le E^{\text{max}}_{i}
\end{equation}

\paragraph{User Satisfaction (Hard Constraint).} The desired energy level must be met at the time of departure. This is modeled as a hard constraint, reflecting a non-negotiable service requirement.
\begin{equation}
E_{i,k_{\text{dep}}} \ge E^{\text{des}}_{i}
\end{equation}
where $k_{\text{dep}}$ is the predicted departure step of the EV within the horizon.

\paragraph{Transformer Power Limit.} The total net power drawn from (or injected into) the grid by all charging stations must not exceed the transformer's maximum capacity.
\begin{equation}
\sum_{i \in \text{CS}} (P^{\text{ch}}_{i,k} - P^{\text{dis}}_{i,k}) \le P^{\text{tr,max}}
\end{equation}

\subsubsection{Problem Classification}
In the field of Operations Research, a Mixed-Integer Linear Program (MILP) is a powerful modeling tool for optimization problems involving complex decisions. A problem is classified as a MILP if it seeks to optimize a linear objective function, subject to a set of linear equality and inequality constraints, where the decision variables can be a mix of continuous and integer values. The general mathematical formulation of a MILP can be expressed as follows:
\begin{equation}
\begin{aligned}
& \underset{x, y}{\text{minimize}} & & c^T x + h^T y \\
& \text{subject to} & & Ax + Gy \leq b \\
& & & x \in \mathbb{Z}^n, \quad y \in \mathbb{R}^p
\end{aligned}
\end{equation}
where $x$ represents the vector of integer variables and $y$ represents the vector of continuous variables. The vectors $c$ and $h$ contain the objective function coefficients, while $A$, $G$, and $b$ define the linear constraints of the system. The requirement for some variables to be integers ($x \in \mathbb{Z}^n$) is what makes MILPs fundamentally different from standard Linear Programs (LPs) and significantly more challenging to solve, forming a core part of the field of combinatorial optimization.

Based on this definition, the mathematical structure of the optimization problem described above is a classic \textbf{Mixed-Integer Linear Program (MILP)}.
This classification is justified as follows:
\begin{itemize}
    \item \textbf{Linear Objective Function:} The objective function is a linear combination of the continuous power variables $P^{\text{ch}}$ and $P^{\text{dis}}$.
    \item \textbf{Linear Constraints:} All system constraints, including energy dynamics, power limits, and state of energy bounds, are formulated as linear equations or inequalities.
    \item \textbf{Mixed-Integer Variables:} The formulation employs both continuous variables (e.g., $P^{\text{ch}}_{i,k}$, $E_{i,k}$) and discrete, binary integer variables ($z_{i,k}$). The binary variables are essential for modeling the logical decision to either charge or discharge at any given time step, but not both simultaneously.
\end{itemize}
The problem is not a Quadratic Program (QP) or Mixed-Integer Quadratic Program (MIQP) because the objective function does not contain any quadratic terms (e.g., minimizing the square of power). Similarly, it is not a Quadratically Constrained Quadratic Program (QCQP) as all constraints are linear. The \texttt{OnlineMPC\_Solver} is therefore designed to solve this specific MILP formulation at each control step.


\section{Approximate Explicit MPC: A Machine Learning Approach}
The online, implicit MPC formulation provides high-quality control decisions by solving an optimization problem at every time step. However, this approach has a significant drawback: its computational complexity. For scenarios with a large number of EVs or a long control horizon, solving a Mixed-Integer Linear Program (MILP) in real-time can be prohibitively slow, making it impractical for many real-world applications.

To overcome this limitation, this work implements an \textbf{Approximate Explicit Model Predictive Controller (A-MPC)}. This controller leverages machine learning to replace the computationally expensive online optimization with a fast, lightweight inference step.

\paragraph{Implementation Details}
The A-MPC is implemented in the \texttt{ApproximateExplicitMPC} class, located in the same \texttt{ev2gym/baselines/pulp\_mpc.py} file. It utilizes a pre-trained \texttt{RandomForestRegressor} model from the \textbf{scikit-learn} library. This model is serialized and stored in the \texttt{mpc\_approximator.joblib} file, and it is generated by the dedicated \texttt{train\_mpc\_approximator.py} script, which executes the data generation and offline training steps. During online operation, the controller's \texttt{get\_action} method simply performs a fast inference call to this loaded model.

\subsection{Methodology: From Oracle to Apprentice}
The core idea is to treat the slow but powerful online MPC as an "oracle" or expert teacher. An apprentice model, in this case a \texttt{RandomForestRegressor} from the \texttt{scikit-learn} library, is trained to mimic the oracle's behavior. The process is as follows:
\begin{enumerate}
    \item \textbf{Data Generation:} The online MPC is run across a diverse range of simulated scenarios. At each step, the state of the environment and the corresponding optimal action computed by the MPC are recorded. This creates a large dataset of state-action pairs, where the actions are considered to be the "ground truth" optimal decisions.
    \item \textbf{State Vector Formulation:} The state $s_t$ fed to the machine learning model is a carefully crafted vector that summarizes all necessary information for making a control decision. It is a fixed-size vector composed of:
    \begin{equation}
        s_t = [ \mathbf{SoC}, \mathbf{T}^{\text{rem}}, \mathbf{C}^{\text{ch}}, \mathbf{C}^{\text{dis}} ]^T
    \end{equation}
    where:
    \begin{itemize}
        \item $\mathbf{SoC}$: A vector of the current State of Charge for all charging stations (padded to a maximum size).
        \item $\mathbf{T}^{\text{rem}}$: A vector of the remaining time until departure for each connected EV.
        \item $\mathbf{C}^{\text{ch}}$: A vector of predicted future charging prices over the horizon $H$.
        \item $\mathbf{C}^{\text{dis}}$: A vector of predicted future discharging prices over the horizon $H$.
    \end{itemize}
    \item \textbf{Offline Training:} The \texttt{RandomForestRegressor} model, denoted $f_{\theta}$, is trained offline on this dataset to learn the mapping from a given state $s_t$ to the oracle's action $a_t$. The model's parameters $\theta$ are optimized to minimize the difference between its predicted action and the oracle's action.
    \item \textbf{Online Inference:} Once trained, the A-MPC controller can be deployed. At each time step, it simply constructs the state vector $s_t$ and computes the action via a fast forward pass through the trained model:
    \begin{equation}
        a_t = f_{\theta}(s_t)
    \end{equation}
    This inference step is orders of magnitude faster than solving a MILP, enabling real-time control for large-scale systems.
\end{enumerate}

\section{Lyapunov-based Adaptive Horizon MPC}
While the A-MPC offers a significant speed-up, it is an approximation and may not always match the performance of the fully-fledged online MPC. A second enhancement developed in this work is the \textbf{Lyapunov-based Adaptive Horizon MPC}, which aims to reduce the computational burden of the online MPC while retaining its optimality and stability guarantees. This method represents an essential improvement, creating an intelligent trade-off between computational cost and control performance.

\paragraph{Implementation Details}
This adaptive horizon mechanism is not a separate controller but rather an advanced operational mode of the \texttt{OnlineMPC\_Solver} class. Its logic is integrated directly within the solver's \texttt{get\_action} method and is activated by setting the \texttt{use\_adaptive\_horizon} flag to true during instantiation. When active, the solver performs the Lyapunov stability check after each optimization step and adjusts its internal horizon parameter, \texttt{current\_H}, accordingly for the next iteration.

\subsection{Core Concept: Dynamic Horizon Adjustment}
The key insight is that a long prediction horizon is not always necessary. When the system is in a stable state and far from its operational constraints, a shorter horizon is sufficient for making good decisions. Conversely, when the system is in a complex or critical state (e.g., an EV is close to its departure time but has a low SoC), a longer horizon is needed for careful planning.

This adaptive controller dynamically adjusts its prediction horizon $H_t$ at each step based on the stability of the system, which is formally assessed using a Lyapunov function.

\subsection{Lyapunov Stability for V2G Control}
A Lyapunov function $V(x)$ is a scalar function that can be thought of as a measure of the system's "energy" or deviation from a desired equilibrium state. For the V2G system, we define the state as the vector of energy levels of all connected EVs, $E = [E_1, E_2, \dots, E_N]^T$. The desired state is the vector of desired energy levels at departure, $E^{\text{des}}$. The Lyapunov function is defined as the sum of the squared errors from this desired state:
\begin{equation}
    V(E) = \sum_{i \in \text{EVs}} (E_i - E_i^{\text{des}})^2
\end{equation}
For the system to be stable, the value of this function must decrease at each step, ensuring the system is always progressing towards its goal. This is known as the Lyapunov decrease condition:
\begin{equation}
    V(E_{t+1}) \le V(E_t) - \alpha V(E_t)
\end{equation}
where $E_{t+1}$ is the state at the next time step resulting from the current control action, and $\alpha$ is a small positive constant that sets the minimum required rate of convergence.

\subsection{Horizon Shortening and Extension}
The adaptive MPC algorithm uses this stability condition to govern its horizon length. At each time step $t$:
\begin{enumerate}
    \item The MPC solves the optimization problem using its current horizon, $H_t$.
    \item It calculates the predicted next state $E_{t+1}$ based on the computed optimal action.
    \item It checks if the Lyapunov decrease condition is satisfied.
    \begin{itemize}
        \item \textbf{If Stable:} The condition holds. The controller is performing well. We can afford to reduce the computational load for the next step by shortening the horizon:
        \begin{equation}
            H_{t+1} = \max(H_{\min}, H_t - 1)
        \end{equation}
        \item \textbf{If Not Stable:} The condition is violated. The system requires more careful planning. The horizon for the next step is extended to provide a longer view into the future:
        \begin{equation}
            H_{t+1} = \min(H_{\max}, H_t + 1)
        \end{equation}
    \end{itemize}
\end{enumerate}
This intelligent adjustment makes the online MPC more efficient and practical, reducing computation time during stable periods while retaining the ability to perform deep planning when necessary.
